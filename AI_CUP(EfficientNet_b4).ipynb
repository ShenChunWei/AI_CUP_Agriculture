{"cells":[{"cell_type":"markdown","source":["#Import Packages"],"metadata":{"id":"Mja2lT6lygcx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fapom2X_A8RI"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zUMiPZsEaX38"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from torch import nn\n","from torch import optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, models\n","from torch.utils.data. sampler import SubsetRandomSampler, WeightedRandomSampler\n","from tqdm import tqdm\n","from typing import Counter\n","import os\n","from sklearn.metrics import classification_report"]},{"cell_type":"code","source":["myseed = 2022\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","np.random.seed(myseed)\n","torch.manual_seed(myseed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(myseed)"],"metadata":{"id":"ZJKYXkYhy-vV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Split Data"],"metadata":{"id":"mHLdoelzyj_S"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKND-eDWaa1u"},"outputs":[],"source":["data_dir = './AI_CUP_Agricultural_Data'\n","\n","def load_split_train_test (datadir, valid_size = 0.1, test_size = 0.1) :\n","\n","    #Data augmentation\n","    train_transforms = transforms.Compose ([transforms.Resize((450,450)),\n","                                            transforms.RandomCrop(380),\n","                                            transforms.RandomHorizontalFlip(),\n","                                            transforms.ToTensor(),\n","                                            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                                                 std=[0.229, 0.224, 0.225]),\n","                                            transforms.RandomErasing()])\n","    \n","    test_transforms = transforms. Compose([transforms.Resize((380, 380)),\n","                                           transforms. ToTensor(),\n","                                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                                                 std=[0.229, 0.224, 0.225]),])\n","    \n","    #load data\n","    train_data = datasets.ImageFolder(data_dir, transform=train_transforms)\n","    valid_data = datasets.ImageFolder(data_dir, transform=test_transforms)\n","    test_data = datasets.ImageFolder(data_dir, transform=test_transforms)\n","\n","    #Split train, valid and test\n","    num_train = len(train_data)\n","    indices = list (range (num_train))\n","    split1 = int (np.floor(valid_size * num_train))\n","    split2 = int (np.floor(test_size * num_train))\n","    np.random.shuffle (indices)\n","    train_idx, valid_idx, test_idx = indices [split1 + split2:], indices[:split1], indices[split1:split1 + split2]\n","\n","    train_set = torch.utils.data.Subset(train_data,train_idx)\n","    valid_set = torch.utils.data.Subset(valid_data,valid_idx)\n","    test_set = torch.utils.data.Subset(test_data,test_idx)\n","\n","    #calculate the weight for each class\n","    class_num = []\n","    classes = ['banana', 'bareland', 'carrot', 'corn', 'dragonfruit', 'garlic', 'guava', 'peanut', 'pineapple', 'pumpkin', 'rice', 'soybean', 'sugarcane', 'tomato']\n","    for i in range(len(classes)):\n","        files = os. listdir('./AI_CUP_Agricultural_Data/' + classes[i])\n","        n = len(files)\n","        class_num.append(n)\n","    class_weight = torch.Tensor([len(train_set)/c for c in class_num])\n","\n","    sample_weight = [0] * len(train_set)\n","    for idx, (_, label) in enumerate(tqdm(train_set)):\n","        weight = class_weight[label]\n","        sample_weight[idx] = weight\n","    \n","    sampler = WeightedRandomSampler(weights=sample_weight, num_samples=len(train_set), replacement=True)\n","\n","    #DataLoader\n","    train_loader = DataLoader (train_set, sampler=sampler, batch_size=16 ,num_workers=4)\n","    valid_loader = DataLoader (valid_set, batch_size=16 ,shuffle=False, num_workers=4)\n","    test_loader = DataLoader(test_set, batch_size=16 ,shuffle=False ,num_workers=4)\n","\n","    return train_loader, valid_loader, test_loader\n","\n","\n","train_loader, valid_loader, test_loader = load_split_train_test(data_dir, 0.1, 0.1)"]},{"cell_type":"markdown","source":["#Data visualization"],"metadata":{"id":"2J44n_bkypR2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxX_Ef12ALfd"},"outputs":[],"source":["classes = ['banana', 'bareland', 'carrot', 'corn', 'dragonfruit', 'garlic', 'guava', 'peanut', 'pineapple', 'pumpkin', 'rice', 'soybean', 'sugarcane', 'tomato']\n","\n","\n","def imshow(img):\n","    img = img.permute(1,2,0)\n","    img = torch.clamp(img,0,1) \n","    plt.imshow(img)\n","\n","dataiter = iter(train_loader)\n","images, labels = dataiter.next()\n","\n","fig = plt.figure(figsize=(12, 8))\n","for idx in np.arange(16):\n","    ax = fig.add_subplot(4, 16/4, idx+1, xticks=[], yticks=[])\n","    imshow(images[idx])\n","    ax.set_title(\"{} \".format( classes[labels[idx]]))"]},{"cell_type":"markdown","metadata":{"id":"paKKCsh1gw9d"},"source":["#Creat model and training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"llSWVjG7lHhy"},"outputs":[],"source":["def train(model, trainloader, optimizer, criterion):\n","    # keep track of training loss\n","    train_loss = 0.0\n","    train_correct = 0\n","    \n","    # train the model \n","    model.train()\n","    for data, target in tqdm(trainloader):\n","        # move tensors to GPU if CUDA is available\n","        data, target = data.to(device), target.to(device)\n","        # clear the gradients of all optimized variables\n","        optimizer.zero_grad()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the batch loss\n","        loss = criterion(output, target)\n","        # backward pass: compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","        # perform a single optimization step (parameter update)\n","        optimizer.step()\n","        # update training loss\n","        train_loss += loss.item()*data.size(0)\n","        # update training Accuracy\n","        _, predicted = torch.max(output.data, 1)\n","        train_correct += (predicted == target).sum().item()\n","\n","    return train_loss/len(train_loader.dataset), train_correct/len(train_loader.dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N6_wrJ6elJeh"},"outputs":[],"source":["@torch.no_grad()\n","def test(model, testloader, criterion):\n","    # keep track of validation loss\n","    valid_loss = 0.0\n","    valid_correct = 0\n","\n","    # evaluate the model \n","    model.eval()\n","    for data, target in tqdm(testloader):\n","        # move tensors to GPU if CUDA is available\n","        data, target = data.to(device), target.to(device)\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the batch loss\n","        loss = criterion(output, target)\n","        # update average validation loss \n","        valid_loss += loss.item()*data.size(0)\n","        # update validation Accuracy\n","        _, predicted = torch.max(output.data, 1)\n","        valid_correct += (predicted == target).sum().item()\n","\n","    return valid_loss/len(test_loader.dataset), valid_correct/len(test_loader.dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDNRTfpulMF6"},"outputs":[],"source":["def modeltrain(model, trainloader, validloader, testloader, optimizer, criterion, epochs, save_model_path, earlystop=4):\n","    history = {\n","        'trainloss' : [],\n","        'trainacc' : [],\n","        'validloss' : [],\n","        'validacc' : [],\n","    }\n","    state = {\n","        'epoch' : 0,\n","        'state_dict' : model.state_dict(),\n","        'trainloss' : 10000,\n","        'trainacc' : 0,\n","        'validloss' : 10000,\n","        'validacc' : 0,\n","    }\n","    valid_loss_min = 10000\n","    trigger = 0\n","    for epoch in range(epochs):\n","        print(f'running epoch: {epoch+1} (learning rate : ' + str(optimizer.param_groups[0]['lr']) + ')')\n","        trainloss, trainacc = train(model, trainloader, optimizer, criterion)\n","        validloss, validacc = test(model, validloader, criterion)\n","\n","\n","        # print training/validation statistics \n","        history['trainloss'].append(trainloss)\n","        history['trainacc'].append(trainacc)\n","        history['validloss'].append(validloss)\n","        history['validacc'].append(validacc)\n","        print(f'Training Loss  : {trainloss:.4f}\\t\\tTraining Accuracy  : {trainacc:.4f}')\n","        print(f'Validation Loss: {validloss:.4f}\\t\\tValidation Accuracy: {validacc:.4f}')\n","        \n","        # save model if validation loss has decreased\n","        if validloss <= valid_loss_min:\n","            print(f'Validation loss decreased ({valid_loss_min:.4f} --> {validloss:.4f}).  Saving model ...\\n')\n","            state['epoch'] = epoch\n","            state['state_dict'] = model.state_dict()\n","            state['trainloss'] = trainloss\n","            state['trainacc'] = trainacc\n","            state['validloss'] = validloss\n","            state['validacc'] = validacc\n","\n","            torch.save(state, save_model_path)\n","            valid_loss_min = validloss\n","            trigger = 0\n","        # if model dont improve for 3 times, interupt.\n","        else:\n","            trigger += 1\n","            print(f'Validation loss increased ({valid_loss_min:.4f} --> {validloss:.4f}). Trigger {trigger}/{earlystop}\\n')\n","            if trigger == 3:\n","                optimizer.param_groups[0]['lr'] =  optimizer.param_groups[0]['lr'] / 10\n","            if trigger == earlystop:\n","                break\n","    print('\\nTest Evaluate:')\n","    testloss, testacc = test(model, testloader, criterion)\n","    state['testloss'] = testloss\n","    state['testacc'] = testacc\n","    torch.save(state, save_model_path)\n","    bestepoch = state['epoch']\n","    validloss = state['validloss']\n","    validacc = state['validacc']\n","    print(f'Best model on epoch : {bestepoch}/{epoch}')\n","    print(f'validation loss: {validloss:.4f}\\t\\t validation acc : {validacc:.4f}')\n","    print(f'test loss      : {testloss:.4f}\\t\\t test acc \\t: {testacc:.4f}')\n","    return history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8H6B1FIR7vpp"},"outputs":[],"source":["model = models.efficientnet_b4(pretrained=True)\n","model.classifier._modules['1'] = nn.Linear(1792, 14)\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xc1wzs1-da7a"},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = model.to(device)\n","model.device = device\n","n_epochs = 30\n","lr = 0.0001\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n","save_path = os.path.join('./AI_CUP_Agricultural', 'model_weight_b4.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_xkjfiE0lTbw"},"outputs":[],"source":["history_fintune = modeltrain(\n","        model = model,\n","        trainloader = train_loader,\n","        validloader = valid_loader,\n","        testloader = test_loader,\n","        optimizer = optimizer,\n","        criterion = criterion,\n","        epochs = n_epochs,\n","        save_model_path = save_path\n","        )"]},{"cell_type":"markdown","metadata":{"id":"hBNceB27iLxR"},"source":["#Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EQAWjSNIPFAb"},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","save_path = os.path.join('./AI_CUP_Agricultural', 'model_weight_b4.pth')\n","\n","model = models.efficientnet_b4(pretrained=True)\n","model.classifier._modules['1'] = nn.Linear(1792, 14)\n","\n","## load weight\n","state = torch.load(save_path)\n","model.load_state_dict(state['state_dict'])\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-pb1WWmrL-MU"},"outputs":[],"source":["truth = []\n","predict = []\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = model.to(device)\n","model.device = device\n","model.eval()\n","\n","with torch.no_grad():\n","    for i, (image, label) in enumerate(tqdm(test_loader)):\n","        image, label = image.to(device), label.to(device)\n","        y_hat = model(image)\n","        _, pred = torch.max(y_hat.data, 1)\n","        for i in label.cpu().detach().numpy():\n","            truth.append(i)\n","        for y in pred.cpu().detach().numpy():\n","            predict.append(y)\n","\n","print(classification_report(truth, predict, digits=4))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"AI_CUP(EfficientNet_b4).ipynb","provenance":[{"file_id":"1cCYEFGlJN5peugtpv1UhNKK-OeWhro5m","timestamp":1653239906014},{"file_id":"1iL5zjCO_ZVMpR73tgXo2EqEoHfQMaBSS","timestamp":1652184396852},{"file_id":"1386Z9aFKxZjCq3IKGhFEEuM-Eics_o-m","timestamp":1651845378344},{"file_id":"1_bnprn0Fn6BAwHWJVql0YlchRUBIZD1C","timestamp":1651425195417},{"file_id":"1Lf8nszh-dQIdpWHe7juFPRAiu6OymrZl","timestamp":1651153219708},{"file_id":"1cqTzSqv8k00Pdhgm87BH5NCs4e2WZZKD","timestamp":1648993982205},{"file_id":"1vR_z77iTku15CVPGjkWm2Hq-d20Z40gS","timestamp":1648618438771}],"mount_file_id":"1cCYEFGlJN5peugtpv1UhNKK-OeWhro5m","authorship_tag":"ABX9TyO6wlIb45zvF08JwEEM5rlO"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}